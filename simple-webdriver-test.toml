[providers]
default_provider = "embedded.default"

[providers.embedded.default]
model_type = "llama"
model = "llama-2-7b-chat"
context_length = 2048
model_path = "/tmp/dummy-model.gguf"

[agent]
fallback_default_max_tokens = 2048
enable_streaming = true
timeout_seconds = 30
max_retry_attempts = 1
autonomous_max_retry_attempts = 1
allow_multiple_tool_calls = true
auto_compact = true

[computer_control]
enabled = false
require_confirmation = true
max_actions_per_second = 5

[webdriver]
enabled = true
safari_port = 4444
chrome_port = 9515
browser = "chrome-headless"